{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building text classifier with Differential Privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will train a text classifier with Differential Privacy by taking a model pre-trained on public text data and fine-tuning it for a different task.\n",
    "\n",
    "When training a model with differential privacy, we almost always face a trade-off between model size and accuracy on the task. The exact details depend on the problem, but a rule of thumb is that the fewer parameters the model has, the easier it is to get a good performance with DP.\n",
    "\n",
    "Most state-of-the-art NLP models are quite deep and large (e.g. [BERT-base](https://github.com/google-research/bert) has over 100M parameters), which makes task of training text model on a private datasets rather challenging.\n",
    "\n",
    "One way of addressing this problem is to divide the training process into two stages. First, we will pre-train the model on a public dataset, exposing the model to generic text data. Assuming that the generic text data is public, we will not be using differential privacy at this step. Then, we freeze most of the layers, leaving only a few upper layers to be trained on the private dataset using DP-SGD. This way we can get the best of both worlds - we have a deep and powerful text understanding model, while only training a small number of parameters with differentially private algorithm.\n",
    "\n",
    "In this tutorial we will take the pre-trained [BERT-base](https://github.com/google-research/bert) model and fine-tune it to recognize textual entailment on the [SNLI](https://nlp.stanford.edu/projects/snli/) dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to download the dataset (we'll user Stanford NLP mirror)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "STANFORD_SNLI_URL = \"https://nlp.stanford.edu/projects/snli/snli_1.0.zip\"\n",
    "DATA_DIR = \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and extracting ...\n",
      "Completed!\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import urllib.request\n",
    "import os\n",
    "\n",
    "def download_and_extract(dataset_url, data_dir):\n",
    "    print(\"Downloading and extracting ...\")\n",
    "    filename = \"snli.zip\"\n",
    "    urllib.request.urlretrieve(dataset_url, filename)\n",
    "    with zipfile.ZipFile(filename) as zip_ref:\n",
    "        zip_ref.extractall(data_dir)\n",
    "    os.remove(filename)\n",
    "    print(\"Completed!\")\n",
    "\n",
    "download_and_extract(STANFORD_SNLI_URL, DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset comes in two formats (`tsv` and `json`) and has already been split into train/dev/test. Let’s verify that’s the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.DS_Store',\n",
       " 'Icon\\r',\n",
       " 'README.txt',\n",
       " 'snli_1.0_dev.jsonl',\n",
       " 'snli_1.0_dev.txt',\n",
       " 'snli_1.0_test.jsonl',\n",
       " 'snli_1.0_test.txt',\n",
       " 'snli_1.0_train.jsonl',\n",
       " 'snli_1.0_train.txt']"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "bento_obj_id": "140183721617760"
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snli_folder = os.path.join(DATA_DIR, \"snli_1.0\")\n",
    "os.listdir(snli_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now take a look inside. [SNLI dataset](https://nlp.stanford.edu/projects/snli/) provides ample syntactic metadata, but we'll only use raw input text. Therefore, the only fields we're interested in are **sentence1** (premise), **sentence2** (hypothesis) and **gold_label** (label chosen by the majority of annotators).\n",
    "\n",
    "Label defines the relation between premise and hypothesis: either *contradiction*, *neutral* or *entailment*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.dataresource+json": {
       "data": [
        {
         "gold_label": "neutral",
         "index": 0,
         "sentence1": "A person on a horse jumps over a broken down airplane.",
         "sentence2": "A person is training his horse for a competition."
        },
        {
         "gold_label": "contradiction",
         "index": 1,
         "sentence1": "A person on a horse jumps over a broken down airplane.",
         "sentence2": "A person is at a diner, ordering an omelette."
        },
        {
         "gold_label": "entailment",
         "index": 2,
         "sentence1": "A person on a horse jumps over a broken down airplane.",
         "sentence2": "A person is outdoors, on a horse."
        },
        {
         "gold_label": "neutral",
         "index": 3,
         "sentence1": "Children smiling and waving at camera",
         "sentence2": "They are smiling at their parents"
        },
        {
         "gold_label": "entailment",
         "index": 4,
         "sentence1": "Children smiling and waving at camera",
         "sentence2": "There are children present"
        }
       ],
       "schema": {
        "fields": [
         {
          "name": "index",
          "type": "integer"
         },
         {
          "name": "sentence1",
          "type": "string"
         },
         {
          "name": "sentence2",
          "type": "string"
         },
         {
          "name": "gold_label",
          "type": "string"
         }
        ],
        "pandas_version": "0.20.0",
        "primaryKey": [
         "index"
        ]
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>gold_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A person on a horse jumps over a broken down a...</td>\n",
       "      <td>A person is training his horse for a competition.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A person on a horse jumps over a broken down a...</td>\n",
       "      <td>A person is at a diner, ordering an omelette.</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A person on a horse jumps over a broken down a...</td>\n",
       "      <td>A person is outdoors, on a horse.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Children smiling and waving at camera</td>\n",
       "      <td>They are smiling at their parents</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Children smiling and waving at camera</td>\n",
       "      <td>There are children present</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sentence1  \\\n",
       "0  A person on a horse jumps over a broken down a...   \n",
       "1  A person on a horse jumps over a broken down a...   \n",
       "2  A person on a horse jumps over a broken down a...   \n",
       "3              Children smiling and waving at camera   \n",
       "4              Children smiling and waving at camera   \n",
       "\n",
       "                                           sentence2     gold_label  \n",
       "0  A person is training his horse for a competition.        neutral  \n",
       "1      A person is at a diner, ordering an omelette.  contradiction  \n",
       "2                  A person is outdoors, on a horse.     entailment  \n",
       "3                  They are smiling at their parents        neutral  \n",
       "4                         There are children present     entailment  "
      ]
     },
     "execution_count": 6,
     "metadata": {
      "bento_obj_id": "140183951218768"
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train_path =  os.path.join(snli_folder, \"snli_1.0_train.txt\")\n",
    "dev_path = os.path.join(snli_folder, \"snli_1.0_dev.txt\")\n",
    "\n",
    "df_train = pd.read_csv(train_path, sep='\\t')\n",
    "df_test = pd.read_csv(dev_path, sep='\\t')\n",
    "\n",
    "df_train[['sentence1', 'sentence2', 'gold_label']][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT (Bidirectional Encoder Representations from Transformers) is state of the art approach to various NLP tasks. It uses a Transformer architecture and relies heavily on the concept of pre-training. \n",
    "\n",
    "We'll use a pre-trained BERT-base model, provided in huggingface [transformers](https://github.com/huggingface/transformers) repo.\n",
    "It gives us a pytorch implementation for the classic BERT architecture, as well as a tokenizer and weights pre-trained on a public English corpus (Wikipedia).\n",
    "\n",
    "Please follow these [installation instrucitons](https://github.com/huggingface/transformers#installation) before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "model_name = \"bert-base-cased\"\n",
    "config = BertConfig.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=3,\n",
    ")\n",
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    \"bert-base-cased\",\n",
    "    do_lower_case=False,\n",
    ")\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-cased\",\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has the following structure. It uses a combination of word, positional and token *embeddings* to create a sequence representation, then passes the data through 12 *transformer encoders* and finally uses a *linear classifier* to produce the final label.\n",
    "As the model is already pre-trained and we only plan to fine-tune few upper layers, we want to freeze all layers, except for the last encoder and above (`BertPooler` and `Classifier`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/BERT.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters count: 108312579\n",
      "Trainable parameters count: 7680771\n"
     ]
    }
   ],
   "source": [
    "trainable_layers = [model.bert.encoder.layer[-1], model.bert.pooler, model.classifier]\n",
    "total_params = 0\n",
    "trainable_params = 0\n",
    "\n",
    "for p in model.parameters():\n",
    "        p.requires_grad = False\n",
    "        total_params += p.numel()\n",
    "\n",
    "for layer in trainable_layers:\n",
    "    for p in layer.parameters():\n",
    "        p.requires_grad = True\n",
    "        trainable_params += p.numel()\n",
    "\n",
    "print(f\"Total parameters count: {total_params}\") # ~108M\n",
    "print(f\"Trainable parameters count: {trainable_params}\") # ~7M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, by using pre-trained model we reduce the number of trainable params from over 100 millions to just above 7.5 millions. This will help both performance and convergence with added noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin training, we need to preprocess the data and convert it to the format our model expects. \n",
    "\n",
    "(Note: it'll take 5-10 minutes to run on a laptop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_LIST = ['contradiction', 'entailment', 'neutral']\n",
    "MAX_SEQ_LENGHT = 128\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from torch.utils.data import TensorDataset\n",
    "from transformers.data.processors.utils import InputExample\n",
    "from transformers.data.processors.glue import glue_convert_examples_to_features\n",
    "\n",
    "\n",
    "def _create_examples(df, set_type):\n",
    "    \"\"\" Convert raw dataframe to a list of InputExample. Filter malformed examples\n",
    "    \"\"\"\n",
    "    examples = []\n",
    "    for index, row in df.iterrows():\n",
    "        if row['gold_label'] not in LABEL_LIST:\n",
    "            continue\n",
    "        if not isinstance(row['sentence1'], str) or not isinstance(row['sentence2'], str):\n",
    "            continue\n",
    "            \n",
    "        guid = f\"{index}-{set_type}\"\n",
    "        examples.append(\n",
    "            InputExample(guid=guid, text_a=row['sentence1'], text_b=row['sentence2'], label=row['gold_label']))\n",
    "    return examples\n",
    "\n",
    "def _df_to_features(df, set_type):\n",
    "    \"\"\" Pre-process text. This method will:\n",
    "    1) tokenize inputs\n",
    "    2) cut or pad each sequence to MAX_SEQ_LENGHT\n",
    "    3) convert tokens into ids\n",
    "    \n",
    "    The output will contain:\n",
    "    `input_ids` - padded token ids sequence\n",
    "    `attention mask` - mask indicating padded tokens\n",
    "    `token_type_ids` - mask indicating the split between premise and hypothesis\n",
    "    `label` - label\n",
    "    \"\"\"\n",
    "    examples = _create_examples(df, set_type)\n",
    "    \n",
    "    #backward compatibility with older transformers versions\n",
    "    legacy_kwards = {}\n",
    "    from packaging import version\n",
    "    if version.parse(transformers.__version__) < version.parse(\"2.9.0\"):\n",
    "        legacy_kwards = {\n",
    "            \"pad_on_left\": False,\n",
    "            \"pad_token\": tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0],\n",
    "            \"pad_token_segment_id\": 0,\n",
    "        }\n",
    "    \n",
    "    return glue_convert_examples_to_features(\n",
    "        examples=examples,\n",
    "        tokenizer=tokenizer,\n",
    "        label_list=LABEL_LIST,\n",
    "        max_length=MAX_SEQ_LENGHT,\n",
    "        output_mode=\"classification\",\n",
    "        **legacy_kwards,\n",
    "    )\n",
    "\n",
    "def _features_to_dataset(features):\n",
    "    \"\"\" Convert features from `_df_to_features` into a single dataset\n",
    "    \"\"\"\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "    all_attention_mask = torch.tensor(\n",
    "        [f.attention_mask for f in features], dtype=torch.long\n",
    "    )\n",
    "    all_token_type_ids = torch.tensor(\n",
    "        [f.token_type_ids for f in features], dtype=torch.long\n",
    "    )\n",
    "    all_labels = torch.tensor([f.label for f in features], dtype=torch.long)\n",
    "    dataset = TensorDataset(\n",
    "        all_input_ids, all_attention_mask, all_token_type_ids, all_labels\n",
    "    )\n",
    "\n",
    "    return dataset\n",
    "\n",
    "train_features = _df_to_features(df_train, \"train\")\n",
    "test_features = _df_to_features(df_test, \"test\")\n",
    "\n",
    "train_dataset = _features_to_dataset(train_features)\n",
    "test_dataset = _features_to_dataset(test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing batch size\n",
    "\n",
    "Let's talk about batch sizes for a bit.\n",
    "\n",
    "In addition to all the considerations you normally take into account when choosing batch size, training model with DP adds another one - privacy cost. \n",
    "\n",
    "Because of the threat model we assume and the way we add noise to the gradients, larger batch sizes (to a certain extent) generally help convergence. We add the same amount of noise to each gradient update (scaled to the norm of one sample in the batch) regardless of the batch size. What this means is that as the batch size increases, the relative amount of noise added decreases. while preserving the same epsilon guarantee. \n",
    "\n",
    "You should, however, keep in mind that increasing batch size has its price in terms of epsilon, which grows at `O(sqrt(batch_size))` as we train (therefore larger batches make it grow faster). The good strategy here is to experiment with multiple combinations of `batch_size` and `noise_multiplier` to find the one that provides best possible quality at acceptable privacy guarantee.\n",
    "\n",
    "There's another side to this - memory. Opacus computes and stores *per sample* gradients, so for every normal gradient, Opacus will store `n=batch_size` per-sample gradients on each step, thus increasing the memory footprint by at least `O(batch_size)`. In reality, however, the peak memory requirement is `O(batch_size^2)` compared to non-private model. This is because some intermediate steps in per sample gradient computation involve operations on two matrices, each with batch_size as one of the dimensions.\n",
    "\n",
    "The good news is, we can pick the most appropriate batch size, regardless of memory constrains. Opacus has built-in support for *virtual* batches. Using it we can separate physical steps (gradient computation) and logical steps (noise addition and parameter updates): use larger batches for training, while keeping memory footprint low. Below we will specify two constants:\n",
    "\n",
    "- `BATCH_SIZE` defines the maximum batch size we can afford from a memory standpoint, and only affects computation speed\n",
    "- `VIRTUAL_BATCH_SIZE`, on the other hand, is equivalent to normal batch_size in the non-private setting, and will affect convergence and privacy guarantee.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "VIRTUAL_BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "test_sampler = SequentialSampler(test_dataset)\n",
    "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Move the model to appropriate device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, eps=1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will define and attach PrivacyEngine. There are two parameters you need to consider here:\n",
    "\n",
    "- `noise_multiplier`. It defines the trade-off between privacy and accuracy. Adding more noise will provide stronger privacy guarantees, but will also hurt model quality.\n",
    "- `max_grad_norm`. Defines the maximum magnitude of L2 norms to which we clip per sample gradients. There is a bit of tug of war with this threshold: on the one hand, a low threshold means that we will clip many gradients, hurting convergence, so we might be tempted to raise it. However, recall that we add noise with `std=noise_multiplier * max_grad_norm` so we will pay for the increased threshold with more noise. In most cases you can rely on the model being quite resilient to clipping (after the first few iterations your model will tend to adjust so that its gradients stay below the clipping threshold), so you can often just keep the default value (`=1.0`) and focus on tuning `batch_size` and `noise_multiplier` instead. That being said, sometimes clipping hurts the model so it may be worth experimenting with different clipping thresholds, like we are doing in this tutorial.\n",
    "\n",
    "These two parameters define the scale of the noise we add to gradients: the noise will be sampled from a Gaussian distribution with `std=noise_multiplier * max_grad_norm`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opacus import PrivacyEngine\n",
    "\n",
    "ALPHAS = [1 + x / 10.0 for x in range(1, 100)] + list(range(12, 64))\n",
    "NOISE_MULTIPLIER = 0.4\n",
    "MAX_GRAD_NORM = 0.1\n",
    "\n",
    "privacy_engine = PrivacyEngine(\n",
    "    module=model,\n",
    "    batch_size=VIRTUAL_BATCH_SIZE,\n",
    "    sample_size=len(train_dataset),\n",
    "    alphas=ALPHAS,\n",
    "    noise_multiplier=NOISE_MULTIPLIER,\n",
    "    max_grad_norm=MAX_GRAD_NORM,\n",
    ")\n",
    "privacy_engine.attach(optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s first define the evaluation cycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def accuracy(preds, labels):\n",
    "    return (preds == labels).mean()\n",
    "\n",
    "# define evaluation cycle\n",
    "def evaluate(model):    \n",
    "    model.eval()\n",
    "\n",
    "    loss_arr = []\n",
    "    accuracy_arr = []\n",
    "    \n",
    "    for batch in test_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inputs = {'input_ids':      batch[0],\n",
    "                      'attention_mask': batch[1],\n",
    "                      'token_type_ids': batch[2],\n",
    "                      'labels':         batch[3]}\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            loss, logits = outputs[:2]\n",
    "            \n",
    "            preds = np.argmax(logits.detach().cpu().numpy(), axis=1)\n",
    "            labels = inputs['labels'].detach().cpu().numpy()\n",
    "            \n",
    "            loss_arr.append(loss.item())\n",
    "            accuracy_arr.append(accuracy(preds, labels))\n",
    "    \n",
    "    model.train()\n",
    "    return np.mean(loss_arr), np.mean(accuracy_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we specify the training parameters and run the training loop for three epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 3\n",
    "LOGGING_INTERVAL = 1000 # once every how many steps we run evaluation cycle and report metrics\n",
    "DELTA = 1 / len(train_dataloader) # Parameter for privacy accounting. Probability of not uploding privacy guarantees\n",
    "\n",
    "\n",
    "assert VIRTUAL_BATCH_SIZE % BATCH_SIZE == 0 # VIRTUAL_BATCH_SIZE should be divisible by BATCH_SIZE\n",
    "virtual_batch_rate = VIRTUAL_BATCH_SIZE / BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, EPOCHS+1):\n",
    "    losses = []\n",
    "    for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "        \n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'token_type_ids': batch[2],\n",
    "                  'labels':         batch[3]}\n",
    "\n",
    "        outputs = model(**inputs) # output = loss, logits, hidden_states, attentions\n",
    "        \n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # We process small batches of size BATCH_SIZE, \n",
    "        # until they're accumulated to a batch of size VIRTUAL_BATCH_SIZE.\n",
    "        # Only then we make a real `.step()` and update model weights\n",
    "        if (step + 1) % virtual_batch_rate == 0 or step == len(train_dataloader) - 1:\n",
    "            optimizer.step()\n",
    "        else:\n",
    "            optimizer.virtual_step()\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        if step > 0 and step % LOGGING_INTERVAL == 0:\n",
    "            train_loss = np.mean(losses)\n",
    "            eps, alpha = optimizer.privacy_engine.get_privacy_spent(DELTA)\n",
    "\n",
    "            eval_loss, eval_accuracy = evaluate(model)\n",
    "\n",
    "            print(\n",
    "                f\"Epoch: {epoch} | \"\n",
    "                f\"Step: {step} | \"\n",
    "                f\"Train loss: {train_loss:.3f} | \"\n",
    "                f\"Eval loss: {eval_loss:.3f} | \"\n",
    "                f\"Eval accuracy: {eval_accuracy:.3f} | \"\n",
    "                f\"ɛ: {eps:.2f} (α: {alpha})\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the test accuracy, after training for three epochs you should expect something close to the results below.\n",
    "\n",
    "You can see that we can achieve quite strong privacy guarantee at epsilon=7.5 with a moderate accuracy cost of 11 percentage points compared to non-private model trained in a similar setting (upper layers only) and 16 points compared to best results we were able to achieve using the same architecture.\n",
    "\n",
    "*NB: When not specified, DP-SGD is trained with upper layers only*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "| Model | Noise multiplier | Batch size | Accuracy | Epsilon |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| no DP, train full model | N/A | 32 | 90.1% | N/A |\n",
    "| no DP, train upper layers only | N/A | 32 | 85.4% | N/A |\n",
    "| DP-SGD | 1.0 | 32 | 70.5% | 0.7 |\n",
    "| **DP-SGD (this tutorial)** | **0.4** | **32** | **74.3%** | **7.5** |\n",
    "| DP-SGD | 0.3 | 32 | 75.8% | 20.7 |\n",
    "| DP-SGD | 0.1 | 32 | 78.3% | 2865 |\n",
    "| DP-SGD | 0.4 | 8 | 67.3% | 5.9 |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
